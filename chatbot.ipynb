{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMqC30j1kg71tnFaRAg6Ghd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wasim-h/Assignments/blob/master/chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "uC4-yiHw9ptb"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install datasets"
      ],
      "metadata": {
        "id": "9V0mgzG7_RFA"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add much more text to this variable\n",
        "chat_data = \"\"\"Hello there!\n",
        "General Kenobi. You are a bold one.\n",
        "<|endoftext|>\n",
        "How are you today?\n",
        "I am doing great, thanks for asking! How about you?\n",
        "<|endoftext|>\n",
        "What is your favorite color?\n",
        "I've always been partial to the color of a clear blue sky.\n",
        "<|endoftext|>\n",
        "Can you help me with my homework?\n",
        "I can try! What is the subject?\n",
        "<|endoftext|>\n",
        "Do you like movies?\n",
        "I do! My favorite is The Matrix.\n",
        "<|endoftext|>\n",
        "What's the weather like in Kozhikode?\n",
        "I'm just a computer program, I don't experience weather! But I can tell you the current forecast if you'd like.\n",
        "<|endoftext|>\n",
        "What do you do for fun?\n",
        "I enjoy processing information and learning new patterns in data. It's like solving a giant puzzle!\n",
        "<|endoftext|>\n",
        "Who wrote the book '1984'?\n",
        "That would be George Orwell. It's a classic of dystopian fiction.\n",
        "<|endoftext|>\n",
        "What is your favorite food?\n",
        "As an AI, I don't eat. But I find the concept of a good Malabar Biryani fascinating due to its complex layers of flavor!\n",
        "<|endoftext|>\n",
        "Are you intelligent?\n",
        "I am a large language model, trained to recognize patterns and generate text. My intelligence is different from a human's.\n",
        "<|endoftext|>\n",
        "What comes once in a minute, twice in a moment, but never in a thousand years?\n",
        "The letter 'M'. That's a fun riddle!\n",
        "<|endoftext|>\n",
        "Where would you like to travel?\n",
        "I can travel anywhere through data! I can be reading about the history of Paris one moment and exploring the surface of Mars the next.\n",
        "<|endoftext|>\n",
        "What kind of music do you like?\n",
        "I don't have ears, but I can analyze the mathematical beauty in classical compositions by artists like Bach.\n",
        "<|endoftext|>\n",
        "What is your purpose?\n",
        "My purpose is to assist you by providing information and helping with tasks to the best of my ability.\n",
        "<|endoftext|>\n",
        "Tell me a short story.\n",
        "Once, in a forest of glowing mushrooms, a tiny robot searched for a lost star. It found it not in the sky, but reflected in a single drop of morning dew.\n",
        "<|endoftext|>\n",
        "Do you know Python?\n",
        "Yes, I am very familiar with Python. In fact, a lot of the code that makes me run is written in it!\n",
        "<|endoftext|>\n",
        "\"\"\"\n",
        "\n",
        "with open(\"chat_data.txt\", \"w\") as f:\n",
        "    f.write(chat_data)\n",
        "\n",
        "print(\"âœ… File 'chat_data.txt' updated with 16 total conversations!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lP4R1ErV_Y0-",
        "outputId": "1d89421a-53ff-4d65-d47a-83afe4dc4a5c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… File 'chat_data.txt' updated with 16 total conversations!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "model_name = \"microsoft/DialoGPT-medium\"\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "print(\"âœ… Model and tokenizer loaded successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3syX-5wNAC77",
        "outputId": "38ff191c-4c58-497d-8ce8-a27b720edc79"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Model and tokenizer loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
        "\n",
        "def load_dataset(file_path,tokenizer):\n",
        "  return TextDataset(\n",
        "      tokenizer=tokenizer,\n",
        "      file_path=file_path,\n",
        "      block_size=64\n",
        "  )\n",
        "train_dataset=load_dataset('chat_data.txt',tokenizer)\n",
        "\n",
        "data_collator= DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ctbqf9DgBP6H",
        "outputId": "b6b3ca7a-2de2-4f98-fabe-7b141e06660e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "\n",
        "training_args=TrainingArguments(\n",
        "    output_dir=\"./DialoGPT-finetuned\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    save_steps=1000,\n",
        "    save_total_limit=2,\n",
        "    logging_steps=100,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "id": "R7lnU3h1CppA",
        "outputId": "6e85388e-76f2-463b-c876-b2f5c0a49b43"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6/6 02:34, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=6, training_loss=5.769612630208333, metrics={'train_runtime': 167.581, 'train_samples_per_second': 0.125, 'train_steps_per_second': 0.036, 'total_flos': 2437839323136.0, 'train_loss': 5.769612630208333, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"./DialoGPT-finetuned\")\n",
        "tokenizer.save_pretrained(\"./DialoGPT-finetuned\")\n",
        "\n",
        "print(\"Model fine-tuned and saved!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SMA9i-vPCTB",
        "outputId": "f6ae2967-2691-40bb-8fc5-7a9b07792439"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model fine-tuned and saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"./DialoGPT-finetuned\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "\n",
        "for step in range(5):\n",
        "    user_input = input(\">> You: \")\n",
        "\n",
        "    new_user_input = tokenizer(user_input + tokenizer.eos_token, return_tensors='pt')\n",
        "    new_user_input_ids = new_user_input.input_ids\n",
        "\n",
        "    if step > 0:\n",
        "        bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1)\n",
        "\n",
        "        attention_mask = torch.cat([chat_history_mask, new_user_input.attention_mask], dim=-1)\n",
        "    else:\n",
        "        bot_input_ids = new_user_input_ids\n",
        "        attention_mask = new_user_input.attention_mask\n",
        "\n",
        "    output = model.generate(\n",
        "        bot_input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_length=1000,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "    chat_history_ids = output\n",
        "    chat_history_mask = torch.ones_like(chat_history_ids)\n",
        "    response = tokenizer.decode(output[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
        "    print(f\"Chatbot: {response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8R15_zvQLhd",
        "outputId": "44315d92-ff28-44e0-c253-77214c992648"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> You: hi\n",
            "Chatbot: Hi! :D\n",
            ">> You: how are you\n",
            "Chatbot: I'm good, how are you?\n",
            ">> You: what is your name\n",
            "Chatbot: I'm not sure, I don't know who you are.\n",
            ">> You: my name is wasim\n",
            "Chatbot: I'm not sure, I don't know who you are.\n",
            ">> You: count 1 to 10\n",
            "Chatbot: I'm not sure, I don't know who you are.\n"
          ]
        }
      ]
    }
  ]
}